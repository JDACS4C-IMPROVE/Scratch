
= High Error Drug (HED) Workflow

== Quick start

. `upf-uno-heds.sh` is the main user script.
. Obtain data, set this location as `DATA_SOURCE` in `upf-uno-heds.sh` .
. Set output location `CANDLE_DATA_DIR` in `upf-uno-heds.sh` .
. Set the software locations in `upf-uno-heds.sh` .
. Set system settings in `cfg-sys-1.sh` (or in the environment).
. Make your `UPF`, possibly using `gen-json`.  This is the list of hyperparameters that vary by run.
. Edit your `DFLTS` file as necessary.  This is the list of hyperparameters that do not vary by run.
. Run with:
+
----
$ ./upf-uno-heds.sh aurora UPF DFLTS
----

== Feature overview

. Uses IMPROVE-compliant data and CANDLE-compliant model.
. Totally parallel, run-anywhere behavior during workflow run.  Any size job can be submitted.
. Checkpointing at workflow (whole RUN) and model training (epoch) level (via CANDLE checkpoint module).
. Parallel data staging to local FS via MPI-IO.
. Parallel software staging to local FS via MPI-IO.
. All data manipulation (`partition_uno_pq`) in local FS.
. Output data organized into EXP and RUN directories.
. Performance monitoring may be enabled using in `model_runner`.
. Use of 12 tiles per node on Aurora, easily reconfigurable via `PPN`.

== Workflow behavior

*Overall idea:*
For one EXP, each line in the `UPF` is merged with the JSON in the `DFLTS` file to create a hyperparameter set for a RUN.  Each `index` in the `UPF` is provided to `partition_uno_pq.py` to punch out a drug and put it in the test set.  Each RUN trains a model on the data set and reports the test scores.

The data flows through the *data locations* described below.  Each EXP runs in a unique EXPID created under `CANDLE_DATA_DIR`.  Each RUN has a unique hyperparameter set that the model uses, and is put in a subdirectory of `CANDLE_DATA_DIR/run`, along with logs, checkpoints, and metadata.

. At workflow startup,
.. the TensorFlow environment is broadcasted to the local FS.
.. all training data is broadcasted from `DATA_SOURCE` to `TMP-ORIGINAL`.
. For each RUN, data is partitioned from `TMP-ORIGINAL` to `TMP-INSTANCE` by `model_runner` -> `hed_setup` -> `partition_uno_pq`.
. Training runs on the data in `TMP-INSTANCE`.
. Outputs go directly to the RUN directory on the PFS.

== Data locations

PFS is the shared, persistent, parallel file system.

`DATA_SOURCE`::
Original data in PFS.  This is a preprocessed IMPROVE data set.

`TMP-ORIGINAL`::
`/tmp/$USER/original`: location that data is loaded into from `DATA_SOURCE` by `hook-leader.tcl` .
Specified by hyperparameter `input_dir` .
Hard-coded in `hook-leader.tcl` and `dflts-*.json` .

`TMP-INSTANCE`::
`/tmp/$USER/index-00N`: location for partitioned data from `partition_uno_pq`.
`hed_setup` changes `input_dir` to this directory.
This is also the hyperparameter `instance_directory` and `output_dir` for the model run.
Set by `hed_setup`.

== Components and controls

`upf-uno-heds.sh`::
The main user interface script.  Controls:
+
. `DATA_SOURCE`
. Software locations for IMPROVE and Uno

`cfg-sys-1.sh`::
System settings.  Controls:
+
. `PROCS`: The process count (number of ranks, including 1 server)
. `PPN`: The processes-per-node number
. `QUEUE`: The queue to use
+
All of these may be set in the user environment as well, which takes precedence.

`UPF`::
(Unrolled Parameter File: A hard-coded list of hyperparameters to run.)  The list of JSON hyperparameter sets to run.  One JSON fragment per line.  Generated by `gen-jsons.py`

`DFLTS`::
The JSON hyperparameters to run for every instance.  Each instance combines this set with a set from the UPF.  Settings include `epochs`, `input_dir`==`TMP-ORIGINAL`.  Could be extended with other model hyperparameters, which will override Uno's `uno_default_model.txt`.

`hed_setup`::
Sets up and tears down the training run.  Called by Supervisor's `model_runner.py`.  No user controls.
+
. Before the run:
.. Calls `partition_uno_pq.py` to partition `rsp_merged.parquet` into `rsp_{train,val,test}_data.parquet` using the `index`.
. Sets up all training data in the `TMP-INSTANCE` location.
.. Sets up the XPU for Aurora
. After the run:
.. Touches the marker file for this `index` to prevent restart
.. Unlinks the `TMP-INSTANCE` files to save space.

`partition_uno_pq`::
Derived from Brettin's `create_uno_h5` module, but 1) modified for IMPROVE Parquet files and 2) packaged as a library for use by Supervisor's `model_runner`.

== Checkpointing

This Uno has the CANDLE `ckpt` module, so models are saved each epoch, about once per hour.  Old models beyond the last 3 epochs are automatically deleted.

To restart from an existing EXP, simply provide:

----
$ ./upf-uno-heds.sh aurora UPF DFLTS EXP
----

A new EXP will be created.  The old EXP will not be modified.  The old EXP RUNs are simply copied into the new EXP.  The Supervisor `model_runner` will skip any completed runs with a `marker` file, and the CANDLE `ckpt` module will automatically restart from any models in the RUNs.

== Aurora

Aurora GPU settings are set in:

. Supervisor `env-aurora.sh`
. `hed_setup`: `cfg_xpu()`

These settings automatically run on any number of GPUs up to 12.  Simply set `PROCS` and `PPN` as described above.

== Installation

On Aurora, you can simply use the Swift/T and Supervisor installations that exist and are coded in `upf-uno-hed.sh`.

Clone the "HED workflow scripts" from `git@github.com:JDACS4C-IMPROVE/Scratch.git` , directory `/hed/` .

Clone Wozniak's fork of IMPROVE-UNO from `git@github.com:j-woz/UNO.git` .  This contains some new features for CANDLE `ckpt` and our inferencing approach.  We are working with Rajeev Jain to merge these back in to Uno.

Specify these locations in the main script `upf-uno-heds.sh`.

== Analysis scripts

Pick an EXP and set:
----
$ D=/path/to/EXP123
----

`shrink-logs.sh`::
Converts the logs `out-*.txt` to `summary-*.txt`, removing TensorFlow junk.
Reduces file size by about 99%.
Run with:
+
----
$ shrink-logs.sh $D/out
----
+

`epochs.sh`::
Report completed epochs for all RUNs.
Requires `summary-*.txt` .
Run with `epochs.sh $D 1` .
Writes result in `$D/epochs.txt` .

`progress.sh`::
Report progress summary for this EXP.
Requires `summary-*.txt` .
Run with `progress.sh $D` .
Writes result in `$D/progress.txt` .

`extract.py`::
Extract the test scores for this EXP.
Run with `extract.py $D` .

`export.sh`::
Export the key logs and results for this EXP into a TGZ.
Run with `export.sh $D` .
Creates `$D/EXP___.tgz` .

`clean-ckpts.sh`::
Remove older checkpoint files, as CANDLE `ckpt` does not remove checkpoints created by prior runs.  Run with:
+
----
$ clean-ckpts.sh $D N
----
+
where `N` is the number of recent checkpoints to retain.  Typically set `N=3`.
